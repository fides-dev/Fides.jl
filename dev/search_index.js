var documenterSearchIndex = {"docs":
[{"location":"#Fides.jl","page":"Home","title":"Fides.jl","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"Fides.jl is a Julia wrapper of the Python package Fides.py, which implements an Interior Trust Region Reflective algorithm for boundary constrained optimization problems based on [1, 2]. Fides targets problems on the form:","category":"page"},{"location":"","page":"Home","title":"Home","text":"min_x in mathbbR^n f(x) quad mathrmsubject  to quad lb leq x leq ub","category":"page"},{"location":"","page":"Home","title":"Home","text":"Where f is a continues at least twice-differentaible function, and lb and ub are the lower and upper bounds respectively.","category":"page"},{"location":"#Highlights","page":"Home","title":"Highlights","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"Boundary-constrained interior trust-region optimization.\nRecursive reflective and truncated constraint management.\nFull and 2D subproblem solution solvers.\nSupports used provided Hessian, and BFGS, DFP, and SR1 Hessian approximations.\nGood performance for parameter estimating Ordinary Differential Equation models [3].","category":"page"},{"location":"","page":"Home","title":"Home","text":"note: Star us on GitHub!\nIf you find the package useful in your work please consider giving us a star on GitHub. This will help us secure funding in the future to continue maintaining the package.","category":"page"},{"location":"#Installation","page":"Home","title":"Installation","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"To install Fides.jl in the Julia REPL enter","category":"page"},{"location":"","page":"Home","title":"Home","text":"julia> ] add Fides","category":"page"},{"location":"","page":"Home","title":"Home","text":"or alternatively","category":"page"},{"location":"","page":"Home","title":"Home","text":"julia> using Pkg; Pkg.add(\"Fides\")","category":"page"},{"location":"","page":"Home","title":"Home","text":"Fides is compatible with Julia version 1.10 and above. For best performance we strongly recommend using the latest Julia version.","category":"page"},{"location":"#Getting-help","page":"Home","title":"Getting help","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"If you have any problems using Fides, here are some helpful tips:","category":"page"},{"location":"","page":"Home","title":"Home","text":"Check the Fides Python documentation.\nPost your questions in the #sciml-sysbio or #math-optimization channel on the Julia Slack.\nIf you have encountered unexpected behavior or a bug, please open an issue on GitHub.","category":"page"},{"location":"#Citation","page":"Home","title":"Citation","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"If you found Fides useful in your work, please cite the following paper:","category":"page"},{"location":"","page":"Home","title":"Home","text":"@article{2022fides,\n  title={Fides: Reliable trust-region optimization for parameter estimation of ordinary differential equation models},\n  author={Fr{\\\"o}hlich, Fabian and Sorger, Peter K},\n  journal={PLoS computational biology},\n  volume={18},\n  number={7},\n  pages={e1010322},\n  year={2022},\n  publisher={Public Library of Science San Francisco, CA USA}\n}","category":"page"},{"location":"#References","page":"Home","title":"References","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"Coleman, T. F., & Li, Y. (1994). On the convergence of interior-reflective Newton methods for nonlinear minimization subject to bounds. Mathematical programming, 67(1), 189-224.\nColeman, T. F., & Li, Y. (1996). An interior trust region approach for nonlinear minimization subject to bounds. SIAM Journal on optimization, 6(2), 418-445.\nFröhlich, F., & Sorger, P. K. (2022). Fides: Reliable trust-region optimization for parameter estimation of ordinary differential equation models. PLoS computational biology, 18(7), e1010322.","category":"page"},{"location":"tutorial/#Tutorial","page":"Tutorial","title":"Tutorial","text":"","category":"section"},{"location":"tutorial/","page":"Tutorial","title":"Tutorial","text":"This overarching tutorial describes how to solve an optimization problem with Fides. It further provides performance tips for computationally intensive objective functions.","category":"page"},{"location":"tutorial/#Input-a-Function-to-Minimize","page":"Tutorial","title":"Input - a Function to Minimize","text":"","category":"section"},{"location":"tutorial/","page":"Tutorial","title":"Tutorial","text":"Fides requires a function to minimize, its gradient and optionally its Hessian. In this tutorial, the nonlinear Rosenbrock function is used:","category":"page"},{"location":"tutorial/","page":"Tutorial","title":"Tutorial","text":"f(x_1 x_2) = (10 - x_1)^2 + 1000(x_2 - x_1^2)^2","category":"page"},{"location":"tutorial/","page":"Tutorial","title":"Tutorial","text":"The objective function is expected to take a vector input  and return a scalar:","category":"page"},{"location":"tutorial/","page":"Tutorial","title":"Tutorial","text":"function f(x)\n    return (1.0 - x[1])^2 + 100.0 * (x[2] - x[1]^2)^2\nend\nnothing # hide","category":"page"},{"location":"tutorial/","page":"Tutorial","title":"Tutorial","text":"In particular, x may be either a Vector or a ComponentVector from ComponentArrays.jl. Fides also requires the gradient, and optionally Hessian function. In this example, for convenience we compute both via automatic differentiation using ForwardDiff.jl:","category":"page"},{"location":"tutorial/","page":"Tutorial","title":"Tutorial","text":"using ForwardDiff\ngrad! = (g, x) -> ForwardDiff.gradient!(g, f, x)\nhess! = (H, x) -> ForwardDiff.hessian!(H, f, x)\nnothing # hide","category":"page"},{"location":"tutorial/","page":"Tutorial","title":"Tutorial","text":"Both the gradient and Hessian functions are expected to be in-place on the form; grad!(g, x) and hess!(H, x).","category":"page"},{"location":"tutorial/#Optimization-with-a-Hessian-Approximation","page":"Tutorial","title":"Optimization with a Hessian Approximation","text":"","category":"section"},{"location":"tutorial/","page":"Tutorial","title":"Tutorial","text":"Given an objective function and its gradient, the optimization is performed in a two-step procedure. First, a FidesProblem is created.","category":"page"},{"location":"tutorial/","page":"Tutorial","title":"Tutorial","text":"using Fides\nlb = [-2.0, -2.0]\nub = [ 2.0,  2.0]\nx0 = [ 2.0,  2.0]\nprob = FidesProblem(f, grad!, x0; lb = lb, ub = ub)","category":"page"},{"location":"tutorial/","page":"Tutorial","title":"Tutorial","text":"Where x0 is the initial guess for parameter estimation, and lb and ub are the lower and upper parameter bounds (defaulting to -Inf and Inf if unspecified). The problem is then minimized by calling solve, and when the Hessian is unavailable or too expensive to compute, a Hessian approximation is chosen during this step:","category":"page"},{"location":"tutorial/","page":"Tutorial","title":"Tutorial","text":"sol = solve(prob, Fides.BFGS()) # hide\nsol = solve(prob, Fides.BFGS())","category":"page"},{"location":"tutorial/","page":"Tutorial","title":"Tutorial","text":"Several Hessian approximations are supported (see the API), and of these BFGS generally performs well. Additional tuning options can be set by providing a FidesOptions struct via the options keyword in solve, and a full list of available options is documented in the API.","category":"page"},{"location":"tutorial/#Optimization-with-a-User-Provided-Hessian","page":"Tutorial","title":"Optimization with a User-Provided Hessian","text":"","category":"section"},{"location":"tutorial/","page":"Tutorial","title":"Tutorial","text":"If the Hessian (or a suitable approximation such as the Gauss–Newton approximation) is available, providing it can improve convergence. To provide a Hessian function to FidesProblem do:","category":"page"},{"location":"tutorial/","page":"Tutorial","title":"Tutorial","text":"prob = FidesProblem(f, grad!, x0; hess! = hess!, lb = lb, ub = ub)\nsol = solve(prob) # hide\nsol = solve(prob)","category":"page"},{"location":"tutorial/","page":"Tutorial","title":"Tutorial","text":"Since a Hessian function is provided, no Hessian approximation needs to be specified.","category":"page"},{"location":"tutorial/#Performance-tip:-Computing-Derivatives-and-Objective-Simultaneously","page":"Tutorial","title":"Performance tip: Computing Derivatives and Objective Simultaneously","text":"","category":"section"},{"location":"tutorial/","page":"Tutorial","title":"Tutorial","text":"Internally, the objective function and its derivatives are computed simultaneously by Fides. Hence, runtime can be reduced if intermediate quantities are reused between the objective and derivative computations. To take advantage of this, a FidesProblem can be created with a function that computes the objective and gradient (and optionally the Hessian) for a given input. For example, when only the gradient is available:","category":"page"},{"location":"tutorial/","page":"Tutorial","title":"Tutorial","text":"function fides_obj(x)\n    obj = f(x)\n    g   = ForwardDiff.gradient(f, x)\n    return (obj, g)\nend\n\nhess = false\nprob = FidesProblem(fides_obj, x0, hess; lb = lb, ub = ub)\nsol = solve(prob, Fides.BFGS()) # hide\nsol = solve(prob, Fides.BFGS())","category":"page"},{"location":"tutorial/","page":"Tutorial","title":"Tutorial","text":"Here, the variable hess indicates whether the objective function also returns the Hessian. When a Hessian function is available, do:","category":"page"},{"location":"tutorial/","page":"Tutorial","title":"Tutorial","text":"function fides_obj(x)\n    obj = f(x)\n    g   = ForwardDiff.gradient(f, x)\n    H   = ForwardDiff.hessian(f, x)\n    return (obj, g, H)\nend\n\nhess = true\nprob = FidesProblem(fides_obj, x0, hess; lb = lb, ub = ub)\nsol = solve(prob) # hide\nsol = solve(prob)","category":"page"},{"location":"tutorial/","page":"Tutorial","title":"Tutorial","text":"In this simple example, no runtime benefit is obtained as not quantities are reused between objective and derivative computations. However, if quantities can be reused (for example, when gradients are computed for ODE models), runtime can be noticeably reduced.","category":"page"},{"location":"API/","page":"API","title":"API","text":"CollapsedDocStrings=true","category":"page"},{"location":"API/#API","page":"API","title":"API","text":"","category":"section"},{"location":"API/#FidesProblem","page":"API","title":"FidesProblem","text":"","category":"section"},{"location":"API/","page":"API","title":"API","text":"To solve an optimization problem with Fides, a FidesProblem must first be created:","category":"page"},{"location":"API/","page":"API","title":"API","text":"FidesProblem","category":"page"},{"location":"API/#Fides.FidesProblem","page":"API","title":"Fides.FidesProblem","text":"FidesProblem(f, grad!, x0; hess! = nothing, lb = nothing, ub = nothing)\n\nOptimization problem to be minimized with the Fides Newton Trust Region optimizer.\n\nArguments\n\nf: The objective function to minimize. Accepts a Vector or ComponentVector as input   and return a scalar.\ngrad!: In-place function to compute the gradient of f on the form grad!(g, x).\nx0: Initial starting point for the optimization. Can be a Vector or ComponentVector   from ComponentArrays.jl.\nhess!: (Optional) In-place function to compute the Hessian of f on the form   hess!(H, x). If not provided, a Hessian approximation method must be selected when   calling solve.\nlb: Lower parameter bounds. Defaults to -Inf if not specified.\nub: Upper parameter bounds. Defaults to Inf if not specified.\n\nSee also solve and FidesOptions.\n\nFidesProblem(fides_obj, x0, hess::Bool; lb = nothing, ub = nothing)\n\nOptimization problem created from a function that computes:\n\nhess = false: Objective and gradient; fides_obj(x) -> (obj, g).\nhess = true: Objective, gradient and Hessian; fides_obj(x) -> (obj, g, H).\n\nInternally, Fides computes the objective function and derivatives simultaneously. Therefore, this constructor is the most runtime-efficient option when intermediate quantities can be reused between the objective and derivative computations.\n\nDescription of Fides method\n\nFides implements an Interior Trust Region Reflective method for boundary-constrained optimization, as described in [1, 2]. Optimization problems on the following form are targeted:\n\nmin_x f(x) quad textst quad lb leq x leq ub\n\nAt each iteration, the Fides approximates the objective function by a second-order model:\n\nmin_x m_k(x) = f(x_k) + nabla f(x_k)^T (x - x_k) + 05 (x - x_k)^T B_k (x - x_k)\nquad textst quad x - x_k leq Delta_k\n\nWhere, Δₖ is the trust region radius reflecting the confidence in the second-order approximation, ∇f(xₖ) is the gradient of f at the current iteration xₖ, and Bₖ is a symmetric positive-semidefinite matrix, that is either the exact Hessian (if hess! is provided) or an approximation.\n\nReferences\n\nColeman, T. F., & Li, Y. (1994). On the convergence of interior-reflective Newton  methods for nonlinear minimization subject to bounds. Mathematical programming, 67(1),  189-224.\nColeman, T. F., & Li, Y. (1996). An interior trust region approach for nonlinear  minimization subject to bounds. SIAM Journal on optimization, 6(2), 418-445.\n\n\n\n\n\n","category":"type"},{"location":"API/","page":"API","title":"API","text":"Thereafter, the FidesProblem is solved using the solve function, which accepts numerous tuning options:","category":"page"},{"location":"API/","page":"API","title":"API","text":"solve\nFidesOptions","category":"page"},{"location":"API/#Fides.solve","page":"API","title":"Fides.solve","text":"solve(prob::FidesProblem, hess_approximation; options = FidesOptions())\n\nSolve the given FidesProblem using the Fides Trust Region method, with the specified hess_approximation method for approximating the Hessian matrix.\n\nA complete list of available Hessian approximations can be found in the API documentation.\n\nSee also FidesOptions.\n\n\n\n\n\nsolve(prob::FidesProblem; options = FidesOptions())\n\nSolve the optimization problem prob with the Fides Trust region method with the user provided Hessian in prob.\n\n\n\n\n\n","category":"function"},{"location":"API/#Fides.FidesOptions","page":"API","title":"Fides.FidesOptions","text":"FidesOptions(; kwargs...)\n\nOptions for the Fides Optimizer.\n\nKeyword arguments\n\nmaxiter = 1000: Maximum number of allowed iterations\nfatol = 1e-8: Absolute tolerance for convergence based on objective (f) value\nfrtol = 1e-8: Relative tolerance for convergence based on objective (f) value\ngatol = 1e-6: Absolute tolerance for convergence based on the gradient\ngrtol = 0.0: Relative tolerance for convergence based on the gradient\nxtol = 0.0: Tolerance for convergence based on x (parameter vector)\nmaxtime = Inf: Maximum amount of wall-time in seconds\nverbose: The logging (verbosity) level of the optimizer. Allowed values are:\nwarning (default): Only warnings are printed.\ninfo: Information is printed for each iterations.\nerror: Only errors are printed.\ndebug: Detailed information is printed, typically only of interest for developers.\nstepback_strategy: Refinement method if proposed step reaches optimization boundary.   Allowed options are:\nreflect (default): Recursive reflections at boundary\nrefine: Perform optimization to refine step\nreflect_single: Single reflection at boundary\nmixed: Mix reflections and truncations\ntrunace: Truncate step at boundary and re-solve\nsubspace_solver: Subspace dimension in which the Trust region subproblem is solved.   Allowed options are:\n2D (default): Two dimensional Newton/Gradient subspace\nscg: CG subspace via Steihaug’s method\nfull: Full on R^n\ndelta_init = 1.0: Initial trust region radius\nmu = 0.25: Acceptance threshold for trust region ratio\neta = 0.75: Trust region increase threshold for trust region ratio\ntheta_max = 0.95: Maximal fraction of step that would hit bounds\ngamma1 = 0.25: Factor by which trust region radius will be decreased\ngamma2 = 2.0: Factor by which trust region radius will be increased\nhistory_file = nothing: Records statistics when set\n\n\n\n\n\n","category":"type"},{"location":"API/","page":"API","title":"API","text":"Results are stored in a FidesSolution struct:","category":"page"},{"location":"API/","page":"API","title":"API","text":"Fides.FidesSolution","category":"page"},{"location":"API/#Fides.FidesSolution","page":"API","title":"Fides.FidesSolution","text":"FidesSolution\n\nSolution information from a Fides optmization run.\n\nFields\n\nxmin: Minimizing parameter vector found by the optimization rung\nfmin: Minimum objective value found by the optimization run\nniterations: Number of iterations for the optimization run\nruntime: Runtime in seconds for the optimization run\nretcode: Return code from the optimization run. Possible values are:\nDELTA_TOO_SMALL: Trust Region Radius too small to proceed\nDID_NOT_RUN: Optimizer did not run\nEXCEEDED_BOUNDARY: Exceeded specified boundaries\nFTOL: Converged according to fval difference\nGTOL: Converged according to gradient norm\nXTOL: Converged according to x difference\nMAXITER: Reached maximum number of allowed iterations\nMAXTIME: Reached maximum runtime\nNOT_FINITE: Encountered non-finite fval/grad/hess\n\n\n\n\n\n","category":"type"},{"location":"API/#Hessian-Approximations","page":"API","title":"Hessian Approximations","text":"","category":"section"},{"location":"API/","page":"API","title":"API","text":"In cases where the Hessian is too expensive or difficult to compute, several Hessian approximations are supported. The BFGS method is often effective:","category":"page"},{"location":"API/","page":"API","title":"API","text":"Fides.BFGS\nFides.SR1\nFides.DFP\nFides.Broyden\nFides.BG\nFides.BB","category":"page"},{"location":"API/#Fides.BFGS","page":"API","title":"Fides.BFGS","text":"BFGS(; init_hess = nothing, enforce_curv_cond::Bool = true)\n\nThe Broyden-Fletcher-Goldfarb-Shanno (BFGS) update strategy is a rank-2 update method that preserves both symmetry and positive-semidefiniteness [1].\n\nKeyword arguments\n\ninit_hess = nothing: Initial Hessian for the update scheme. If provided as a Matrix,   the given matrix is used; if set to nothing (default), the identity matrix is used.\nenforce_curv_cond = true: Whether the update should attempt to preserve positive   definiteness. If true, updates from steps that violate the curvature condition are   discarded.\n\nReferences\n\nNocedal, Jorge, and Stephen J. Wright, eds. Numerical optimization. New York, NY:  Springer New York, 1999.\n\n\n\n\n\n","category":"type"},{"location":"API/#Fides.SR1","page":"API","title":"Fides.SR1","text":"SR1(; init_hess = nothing)\n\nThe Symmetric Rank 1 update strategy as described in [1]. This is a rank 1 update strategy that preserves symmetry but does not preserve positive-semidefiniteness.\n\nKeyword arguments\n\ninit_hess = nothing: Initial Hessian for the update scheme. If provided as a Matrix,   the given matrix is used; if set to nothing (default), the identity matrix is used.\n\nReferences\n\nNocedal, Jorge, and Stephen J. Wright, eds. Numerical optimization (Chapter 6.2). New  York, NY: Springer New York, 1999.\n\n\n\n\n\n","category":"type"},{"location":"API/#Fides.DFP","page":"API","title":"Fides.DFP","text":"DFP(; init_hess = nothing, enforce_curv_cond::Bool = true)\n\nThe Davidon-Fletcher-Powell update strategy [1]. This is a rank 2 update strategy that preserves symmetry and positive-semidefiniteness.\n\nKeyword arguments\n\ninit_hess = nothing: Initial Hessian for the update scheme. If provided as a Matrix,   the given matrix is used; if set to nothing (default), the identity matrix is used.\nenforce_curv_cond = true: Whether the update should attempt to preserve positive   definiteness. If true, updates from steps that violate the curvature condition are   discarded.\n\nReferences\n\nAvriel, M. (2003). Nonlinear programming: analysis and methods. Courier Corporation.\n\n\n\n\n\n","category":"type"},{"location":"API/#Fides.Broyden","page":"API","title":"Fides.Broyden","text":"Broyden(phi; init_hess = nothing, enforce_curv_cond::Bool = true)\n\nThe update scheme, as described in [1], which is a generalization of the BFGS/DFP methods where phi controls the convex combination between the two. This rank-2 update strategy preserves both symmetry and positive-semidefiniteness when 0 ≤ phi ≤ 1.\n\nArguments\n\nphi::AbstractFloat: The convex combination parameter interpolating between BFGS   (phi=0) and DFP (phi=1).\n\nKeyword arguments\n\ninit_hess = nothing: Initial Hessian for the update scheme. If provided as a Matrix,   the given matrix is used; if set to nothing (default), the identity matrix is used.\nenforce_curv_cond = true: Whether the update should attempt to preserve positive   definiteness. If true, updates from steps that violate the curvature condition are   discarded.\n\nReferences\n\nNocedal, Jorge, and Stephen J. Wright, eds. Numerical optimization. New York, NY:  Springer New York, 1999.\n\n\n\n\n\n","category":"type"},{"location":"API/#Fides.BG","page":"API","title":"Fides.BG","text":"BG(; init_hess = nothing}\n\nBroydens “good” method as introduced in [1]. This is a rank 1 update strategy that does not preserve symmetry or positive definiteness.\n\nKeyword arguments\n\ninit_hess = nothing: Initial Hessian for the update scheme. If provided as a Matrix,   the given matrix is used; if set to nothing (default), the identity matrix is used.\n\nReferences\n\nBroyden, C. G. (1965). A class of methods for solving nonlinear simultaneous equations.  Mathematics of computation, 19(92), 577-593.\n\n\n\n\n\n","category":"type"},{"location":"API/#Fides.BB","page":"API","title":"Fides.BB","text":"BB(; init_hess = nothing}\n\nThe Broydens “bad” method as introduced in [1]. This is a rank 1 update strategy that does not preserve symmetry or positive definiteness.\n\nKeyword arguments\n\ninit_hess = nothing: Initial Hessian for the update scheme. If provided as a Matrix,   the given matrix is used; if set to nothing (default), the identity matrix is used.\n\nReferences\n\nBroyden, C. G. (1965). A class of methods for solving nonlinear simultaneous equations.  Mathematics of computation, 19(92), 577-593.\n\n\n\n\n\n","category":"type"}]
}
